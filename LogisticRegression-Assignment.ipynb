{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BT2101 Introduction to Logistic Regression\n",
    "\n",
    "#### Version: Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Goal:\n",
    "\n",
    "In this notebook, we will explore logistic regression using:\n",
    "* Gradient ascent method (because you cannot get closed-form solutions)\n",
    "* Open-source package: `scikit-learn`\n",
    "\n",
    "For the gradient descent method, you will:\n",
    "* Use numpy to write functions\n",
    "* Write a likelihood function\n",
    "* Write a derivative function\n",
    "* Write an output function\n",
    "* Write a gradient ascent function\n",
    "* Add a constant column of 1's as intercept term\n",
    "* Use the gradient ascent function to get regression estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from __future__ import division\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Summary of Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lecture class, we know that a typical logistic regression model of *N* observations and *p* predictors:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Logit}(P(y_{i}=1)) &= log(\\frac{P(y_{i}=1)}{1-P(y_{i}=1)}) \\\\\n",
    "& = \\beta_0 + \\sum_{j=1}^p x_j \\beta_j \\\\\n",
    "& = X\\beta\n",
    "\\end{align}\n",
    "\n",
    "Rewrite the model and we can get this function:\n",
    "\n",
    "$$ P(y_{i}=1) = \\frac{e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}}{1+e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}} $$\n",
    "\n",
    "\n",
    "We aim to **Maximize** the (log-)likelihood function:\n",
    "\n",
    "$$ l(\\beta) = \\prod_{i=1}^{N}[\\frac{e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}}{1+e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}}]^{y_i}[\\frac{1}{1+e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}}]^{(1-y_i)} $$\n",
    "Or\n",
    "$$ ll(\\beta) = \\sum_{i=1}^{N}[-log(1+e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j})+y_{i}(\\beta_{0}+\\sum_{j=1}^{p}x_j\\beta_j)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us **Maximize** the log-likelihood function $ll(\\beta)$ and get derivative with respect to $\\beta_j$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial ll(\\beta)}{\\partial\\beta_j} & = -\\sum_{i=1}^{N}\\frac{e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}}{1+e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}}x_{ij}+\\sum_{i=1}^{N}y_{i}x_{ij} \\\\\n",
    "& = \\sum_{i=1}^{N}(y_{i}-\\frac{e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}}{1+e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}})x_{ij} \\\\\n",
    "& = \\sum_{i=1}^{N}(y_{i}-P(y_{i}=1))x_{ij} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the predicted score $P(y_{i}=1)$ is calculated as $\\frac{e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}}{1+e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j}}=\\frac{1}{1+e^{-(\\beta_{0}+\\sum_{j=1}^{p}x_{j}\\beta_{j})}}$ <br/>\n",
    "\n",
    "Then we can write a predicted score function first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(feature_matrix, weight_vector):\n",
    "    '''This function is used to calculate predicted probability or score.\n",
    "    \n",
    "    Inputs:\n",
    "    1) feature_matrix: A matrix of selected features;\n",
    "    2) weight_vector: A vector of coefficients for selected features;\n",
    "    \n",
    "    Outputs:\n",
    "    1) score: A vector of predicted probabilities or scores   \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    score = 1/(1+np.exp(-np.dot(feature_matrix, weight_vector)))\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97068777],\n",
       "       [0.9999546 ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us have a test\n",
    "features = np.array([[2.0, 1.5], [4.8, 5.2]])\n",
    "weights = np.array([1.0, 1.0])\n",
    "weights = np.ones((2, 1))\n",
    "score(features, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Gradient Ascent\n",
    "### 2.1 Computing Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we compute the derivatives of the log-likelihood function? <br/>\n",
    "\n",
    "We already know that the derivative with respect to $\\beta_j$ is: <br/>\n",
    "\\begin{align*}\n",
    "\\frac{\\partial ll(\\beta)}{\\partial\\beta_j} & = \\sum_{i=1}^{N}(y_{i}-P(y_{i}=1))x_{ij} \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(feature_matrix, error_vector):\n",
    "    '''This function is used to calculate the derivatives for features.\n",
    "    Inputs:\n",
    "    1) feature_matrix: Data matrix of features (j = 0,...,p)\n",
    "    2) error_vector: A vector of errors of N observations    \n",
    "    \n",
    "    Outputs:\n",
    "    1) derive: Derivative for this feature j    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    derive = np.dot(feature_matrix.T, error_vector)\n",
    "    \n",
    "    return derive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True output is: \n",
      "[[1]\n",
      " [0]]\n",
      "Predicted score is: \n",
      "[[0.88079708]\n",
      " [0.99183743]]\n",
      "Error is: \n",
      "[[ 0.11920292]\n",
      " [-0.99183743]]\n",
      "Derivative is: \n",
      "[[-4.52241381]]\n"
     ]
    }
   ],
   "source": [
    "# Let us have a test\n",
    "features = np.array([[2.0], [4.8]])\n",
    "weights = np.ones((1, 1))\n",
    "true_outputs = np.array([[1], [0]])\n",
    "predict_score = score(features, weights)\n",
    "errors = true_outputs - predict_score\n",
    "\n",
    "print (\"True output is: \")\n",
    "print (true_outputs)\n",
    "print (\"Predicted score is: \")\n",
    "print (predict_score)\n",
    "print (\"Error is: \")\n",
    "print (errors)\n",
    "print (\"Derivative is: \")\n",
    "print (derivative(features, errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Computing Log-likehood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture class, we know the log likelihood function is computed as:\n",
    "\n",
    "\\begin{align*}\n",
    "ll(\\beta) & = \\sum_{i=1}^{N}[-log(1+e^{\\beta_0 + \\sum_{j=1}^{p}x_j\\beta_j})+y_{i}(\\beta_{0}+\\sum_{j=1}^{p}x_j\\beta_j)] \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(feature_matrix, weight_vector, output_vector):\n",
    "    '''This function is used to calculate log-likelihood value of given features, output and coefficients\n",
    "    Inputs:\n",
    "    1) feature_matrix: A matrix of selected features\n",
    "    2) weight_vector: A vector of coefficients\n",
    "    3) output_vector: A vector of true outputs\n",
    "    \n",
    "    Outputs:\n",
    "    1) ll: Log-likelihood value\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Linear term: Xbeta\n",
    "    X_beta = np.dot(feature_matrix, weight_vector)\n",
    "    \n",
    "    # Sum term: The term in the sum_up bracket\n",
    "    sum_term = -np.log(1+np.exp(X_beta)) + output_vector*X_beta\n",
    "    \n",
    "    # Get log-likelihood\n",
    "    ll = np.sum(sum_term)    \n",
    "    \n",
    "    return ll       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Procedures for Gradient Ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0: Initialize $ \\beta = (\\beta_{0},\\dots,\\beta_{p}) $; <br/>\n",
    "Step 1: Calculate $ \\beta_{j} \\leftarrow \\beta_{j} + Stepsize\\times(\\sum_{i=1}^{N}(y_{i}-P(y_{i}=1))x_{ij}) $, for j = 1,...,p; <br/>\n",
    "Step 2: If not converged, go back to Step 1; <br/>\n",
    "Step 3: Get $ \\beta $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_GA(initial_weights, feature_matrix, true_output, step_size, tolerance, n_iter):\n",
    "    '''This function is used to iteratively calculate coefficients for logistic regression model using Gradient Aescent.\n",
    "    Inputs:\n",
    "    1) initial_weights: Initial regression coefficients\n",
    "    2) feature_matrix: A matrix of selected features\n",
    "    3) true_output: A vector of true outputs -> [0, 1]\n",
    "    4) step_size: Size of step for each iteration of gradient search\n",
    "    5) tolerance: Indicate converging condition\n",
    "    6) n_iter: Maximum number of iterations\n",
    "    \n",
    "    Outputs:\n",
    "    1) weights: Estimated coefficients.    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    weights = np.array(initial_weights, dtype=np.float64)\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # Calculate predicted scores, or probabilitis of 1's\n",
    "        prediction = score(feature_matrix, weights)\n",
    "        \n",
    "        # Calculate errors\n",
    "        error = true_output - prediction\n",
    "        \n",
    "        # Calculating derivatives for weights\n",
    "        derivative_vector = derivative(feature_matrix, error) \n",
    "        \n",
    "        # Updating weights\n",
    "        weights += step_size * derivative_vector        \n",
    "        \n",
    "        # Converging conditions: L2 norm for derivatives\n",
    "        sum_squared_gradient = np.sum(np.power(derivative_vector,2))\n",
    "        \n",
    "        # Verify whether converging early and Report Log-likelihood values            \n",
    "        if np.sqrt(sum_squared_gradient) < tolerance: \n",
    "            # Calculate log-likelihood\n",
    "            loglike = log_likelihood(feature_matrix, weights, true_output)\n",
    "            \n",
    "            # Print out log-likelihood\n",
    "            print ('Iteration %10d: Log-likelihood = %.4f' % (i, loglike))\n",
    "            \n",
    "            return weights\n",
    "        \n",
    "        elif (i<=20) or (i<=100 and i%10==0) or (i<=1000 and i%100==0) or (i==n_iter):            \n",
    "            # Calculate log-likelihood\n",
    "            loglike = log_likelihood(feature_matrix, weights, true_output)\n",
    "            \n",
    "            # Print out log-likelihood\n",
    "            print ('Iteration %10d: Log-likelihood = %.4f' % (i, loglike))\n",
    "            \n",
    "    \n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Convert Pandas DataFrame to Numpy Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to: <br/>\n",
    "1. Convert Pandas DataFrame into a Numpy Array/Matrix to do internal calculations;\n",
    "2. Augment this Array/Matrix by adding 1's column in the first column, in order to calculate the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(df, feature_names, output_name):\n",
    "    '''This function is used to convert pandas dataframe to numpy array/matrix, and augment it with 1's column as intercept.\n",
    "    Inputs:\n",
    "    1) df: Original data in the format of pandas dataframe\n",
    "    2) feature_names: A list of names of selected features\n",
    "    3) output_name: Name of selected outputs\n",
    "    \n",
    "    Outputs:\n",
    "    1) feature_matrix: Augmented feature matrix\n",
    "    2) output_vector: A vector of true outputs    \n",
    "    \n",
    "    '''    \n",
    "    \n",
    "    # Feature names of Augmented matrix \n",
    "    augment_feature_names = ['intercept'] + feature_names\n",
    "    \n",
    "    # Augmented feature matrix by adding constant 1's as intercept term, and reorder the feature matrix\n",
    "    df['intercept'] = 1 \n",
    "    feature_matrix = df[augment_feature_names]\n",
    "    n, k = feature_matrix.shape # n: number of observations; k: number of weights\n",
    "    \n",
    "    # Convert selected feature matrix and output vector to Numpy Array\n",
    "    feature_matrix = feature_matrix.values.reshape((n, k))\n",
    "    output_vector = df[output_name].values.reshape((n, 1))\n",
    "\n",
    "    return (feature_matrix, output_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Load Dataset and Conduct Estimations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset can be obtained from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Iris).\n",
    "\n",
    "### 4.1 Attribute Information:\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "1. sepal length in cm \n",
    "2. sepal width in cm \n",
    "3. petal length in cm \n",
    "4. petal width in cm \n",
    "5. class: <br/>\n",
    "-- Iris Setosa <br/>\n",
    "-- Iris Versicolour <br/>\n",
    "-- Iris Virginica\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose: 1:class='Iris-setosa'; 0:otherwise\n",
    "### It means we want to predict whether class is 'Iris-setosa' (Y=1) or not (Y=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        class\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import dataset into Python Pandas DataFrame\n",
    "filepath = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "colnames = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "data = pd.read_csv(filepath, header=None, names=colnames, dtype={'sepal_length':np.float64, 'sepal_width':np.float64, 'petal_length':np.float64, 'petal_width':np.float64})\n",
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal_length  sepal_width  petal_length  petal_width\n",
       "count    150.000000   150.000000    150.000000   150.000000\n",
       "mean       5.843333     3.054000      3.758667     1.198667\n",
       "std        0.828066     0.433594      1.764420     0.763161\n",
       "min        4.300000     2.000000      1.000000     0.100000\n",
       "25%        5.100000     2.800000      1.600000     0.300000\n",
       "50%        5.800000     3.000000      4.350000     1.300000\n",
       "75%        6.400000     3.300000      5.100000     1.800000\n",
       "max        7.900000     4.400000      6.900000     2.500000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna() # Drop null values\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output dummy\n",
    "data['Iris_setosa'] = data['class'].map(lambda x: 1 if x=='Iris-setosa' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>Iris_setosa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "      <td>0.472984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal_length  sepal_width  petal_length  petal_width  Iris_setosa\n",
       "count    150.000000   150.000000    150.000000   150.000000   150.000000\n",
       "mean       5.843333     3.054000      3.758667     1.198667     0.333333\n",
       "std        0.828066     0.433594      1.764420     0.763161     0.472984\n",
       "min        4.300000     2.000000      1.000000     0.100000     0.000000\n",
       "25%        5.100000     2.800000      1.600000     0.300000     0.000000\n",
       "50%        5.800000     3.000000      4.350000     1.300000     0.000000\n",
       "75%        6.400000     3.300000      5.100000     1.800000     1.000000\n",
       "max        7.900000     4.400000      6.900000     2.500000     1.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and output\n",
    "feature_list =  ['sepal_width', 'petal_length']\n",
    "output_label = 'Iris_setosa'\n",
    "\n",
    "features = convert_data(data, feature_list, output_label)[0]\n",
    "output = convert_data(data, feature_list, output_label)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set up initial parameters and try gradient ascent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "initial_weights = np.ones((len(feature_list)+1, 1))\n",
    "step_size = 7e-5 \n",
    "tolerance = 2e-9\n",
    "iteration = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration          0: Log-likelihood = -854.7068\n",
      "Iteration          1: Log-likelihood = -831.4416\n",
      "Iteration          2: Log-likelihood = -808.1852\n",
      "Iteration          3: Log-likelihood = -784.9390\n",
      "Iteration          4: Log-likelihood = -761.7047\n",
      "Iteration          5: Log-likelihood = -738.4844\n",
      "Iteration          6: Log-likelihood = -715.2805\n",
      "Iteration          7: Log-likelihood = -692.0960\n",
      "Iteration          8: Log-likelihood = -668.9344\n",
      "Iteration          9: Log-likelihood = -645.7998\n",
      "Iteration         10: Log-likelihood = -622.6974\n",
      "Iteration         11: Log-likelihood = -599.6331\n",
      "Iteration         12: Log-likelihood = -576.6144\n",
      "Iteration         13: Log-likelihood = -553.6500\n",
      "Iteration         14: Log-likelihood = -530.7506\n",
      "Iteration         15: Log-likelihood = -507.9292\n",
      "Iteration         16: Log-likelihood = -485.2014\n",
      "Iteration         17: Log-likelihood = -462.5862\n",
      "Iteration         18: Log-likelihood = -440.1063\n",
      "Iteration         19: Log-likelihood = -417.7895\n",
      "Iteration         20: Log-likelihood = -395.6688\n",
      "Iteration         30: Log-likelihood = -198.0342\n",
      "Iteration         40: Log-likelihood = -91.9918\n",
      "Iteration         50: Log-likelihood = -60.1710\n",
      "Iteration         60: Log-likelihood = -49.9844\n",
      "Iteration         70: Log-likelihood = -45.4959\n",
      "Iteration         80: Log-likelihood = -42.8597\n",
      "Iteration         90: Log-likelihood = -40.9528\n",
      "Iteration        100: Log-likelihood = -39.3869\n",
      "Iteration        200: Log-likelihood = -29.1200\n",
      "Iteration        300: Log-likelihood = -23.0344\n",
      "Iteration        400: Log-likelihood = -19.0276\n",
      "Iteration        500: Log-likelihood = -16.2086\n",
      "Iteration        600: Log-likelihood = -14.1239\n",
      "Iteration        700: Log-likelihood = -12.5222\n",
      "Iteration        800: Log-likelihood = -11.2539\n",
      "Iteration        900: Log-likelihood = -10.2252\n",
      "\n",
      "Intercept  X1 X2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.89483748],\n",
       "       [ 1.06595578],\n",
       "       [-1.57802064]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try estimation with gradient descent\n",
    "coefficients = Logistic_GA(initial_weights, features, output, step_size, tolerance, iteration)\n",
    "print ('\\nIntercept ','X1', 'X2')\n",
    "coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try different `initial_weights`, `step_size`, `tolerance`, or `iteration`.<br/>\n",
    "You can also use different features to run logistic regression model. <br/>\n",
    "Large step size may make gradient search fluctuate too much, which is more difficult to converge. <br/>\n",
    "Small tolerance may increase the number of iterations, but will result in more accurate estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Logistic Classifier\n",
    "\n",
    "In this section, you will:\n",
    "* Use `scikit-learn` to run logistic regression model\n",
    "* Do train-test split\n",
    "* Understand ROC curve and AUC\n",
    "* Understand confusion matrix (TP, FP, TN and FN), and performance measures (e.g., accuracy, precision, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Scikit-Learn\n",
    "The package `scikit-learn` can be found at http://scikit-learn.org/stable/index.html. <br/>\n",
    "Please install the package first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nelsonhalim/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset\n",
    "X = data[['sepal_width', 'petal_length']]\n",
    "y = data['Iris_setosa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do train-test split: 70% train and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=2e-09,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model on train data: Using L2-regularization\n",
    "lr = LogisticRegression(fit_intercept=True, max_iter=1000, tol=2e-9, penalty='l2', C=100, random_state=0)\n",
    "lr.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.13752285] [[ 4.1323274  -5.10560028]]\n"
     ]
    }
   ],
   "source": [
    "# Get coefficients\n",
    "print (lr.intercept_, lr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict outputs for test data\n",
    "y_pred = lr.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[29  0]\n",
      " [ 0 16]]\n",
      "29 0 0 16\n"
     ]
    }
   ],
   "source": [
    "# Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "print(cm)\n",
    "print(TN, FP, FN, TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROC and AUC\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Get predicted scores Pr(y=1): Used as thresholds for calculating TP Rate and FP Rate\n",
    "# lr.classes_\n",
    "score = lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Plot ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, score) # fpr: FP Rate, tpr: TP Rate, thresholds: Pr(y=1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.1])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about Logistic Regression can be found at http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using K-Fold Cross Validation to Do Model Evaluation\n",
    "\n",
    "#### K-Fold Cross Validation:\n",
    "\n",
    "<img src='http://www.scielo.br/img/revistas/jmoea/v16n3//2179-1074-jmoea-16-03-0628-gf04.jpg' width='500'>\n",
    "\n",
    "K-Fold cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a standard binary classification dataset from the UCI machine learning repository \"Haberman's Survival Data Set\" (https://archive.ics.uci.edu/ml/datasets/Haberman's+Survival). The dataset has one binary output and three numeric input variables of varying scales. The details about the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Haberman's+Survival)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset: \n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/haberman/haberman.data\"\n",
    "names = ['age', 'year_operation', 'positive_detected', 'survive']\n",
    "dataframe = pd.read_csv(url, names=names)\n",
    "dataframe['survive'] = dataframe['survive'].map(lambda x:1 if x==1 else 0)  # =1, survive after 5 years; =0, die within 5 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>year_operation</th>\n",
       "      <th>positive_detected</th>\n",
       "      <th>survive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>306.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>306.000000</td>\n",
       "      <td>306.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>52.457516</td>\n",
       "      <td>62.852941</td>\n",
       "      <td>4.026144</td>\n",
       "      <td>0.735294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>10.803452</td>\n",
       "      <td>3.249405</td>\n",
       "      <td>7.189654</td>\n",
       "      <td>0.441899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>44.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>52.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>60.750000</td>\n",
       "      <td>65.750000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>83.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age  year_operation  positive_detected     survive\n",
       "count  306.000000      306.000000         306.000000  306.000000\n",
       "mean    52.457516       62.852941           4.026144    0.735294\n",
       "std     10.803452        3.249405           7.189654    0.441899\n",
       "min     30.000000       58.000000           0.000000    0.000000\n",
       "25%     44.000000       60.000000           0.000000    0.000000\n",
       "50%     52.000000       63.000000           1.000000    1.000000\n",
       "75%     60.750000       65.750000           4.000000    1.000000\n",
       "max     83.000000       69.000000          52.000000    1.000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descriptive analysis\n",
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and output\n",
    "features = ['age', 'year_operation', 'positive_detected']\n",
    "output = 'survive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Do K-fold Cross Validation\n",
    "* Set `n_splits=5`: 5-fold cross validation\n",
    "* Set `random_state` to `12345`\n",
    "\n",
    "Hint: Use `KFold` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=5, random_state=12345, shuffle=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a 5-fold cross validation\n",
    "kf = KFold(n_splits = 5, random_state = 12345)\n",
    "kf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2. Create Multiple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Model 1: Logistic regression with L2 regularization\n",
    "* Model 2: Logistic regression with L1 regularization (Let's use the same parameter values as Model 1, except `penalty`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Logistic regression with L2 regularization\n",
    "model_1 = LogisticRegression(fit_intercept=True, max_iter=1000, tol=2e-9, penalty='l2', C=100, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Logistic regression with L1 regularization (Let's use the same parameter values as Model 1, except penalty)\n",
    "model_2 = LogisticRegression(fit_intercept=True, max_iter=1000, tol=2e-9, penalty='l1', C=100, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3. Conduct K-fold Cross-Validation on These Models\n",
    "\n",
    "Hint: Use `cross_val_score` function\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation on Model 1\n",
    "cv_model_1 = cross_val_score(model_1, # Cross-validation on model_1\n",
    "                             dataframe[features], # Feature matrix\n",
    "                             dataframe[output], # Output vector\n",
    "                             cv=kf, # Cross-validation technique\n",
    "                             scoring='accuracy' # Model performance metrics: accuracy\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation on Model 2\n",
    "cv_model_2 = cross_val_score(model_2,\n",
    "                             dataframe[features],\n",
    "                             dataframe[output],\n",
    "                             cv=kf,\n",
    "                             scoring='accuracy'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4. Report and Evaluate Mean Accuracy of These Models\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/1600/1*J2B_bcbd1-s1kpWOu_FZrg.png' width='700'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracies of Model 1:  [0.80645161 0.67213115 0.67213115 0.7704918  0.73770492]\n",
      "Average CV Accuracy of Model 1:  0.7317821258593337\n",
      "CV Accuracies of Model 2:  [0.80645161 0.67213115 0.67213115 0.7704918  0.75409836]\n",
      "Average CV Accuracy of Model 2:  0.7350608143839238\n"
     ]
    }
   ],
   "source": [
    "# Report average cross-validation accuracy of Model 1\n",
    "print(\"CV Accuracies of Model 1: \", cv_model_1)\n",
    "print(\"Average CV Accuracy of Model 1: \", np.average(cv_model_1))\n",
    "\n",
    "# Report average cross-validation accuracy of Model 2\n",
    "print(\"CV Accuracies of Model 2: \", cv_model_2)\n",
    "print(\"Average CV Accuracy of Model 2: \", np.average(cv_model_2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5. Based on the cross-validation accuacy, which model do your prefer ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 References\n",
    "\n",
    "[1] Jason Brownlee, 2018, [Machine Learning Algorithms from Scratch with Python](https://machinelearningmastery.com/machine-learning-algorithms-from-scratch/). <br/>\n",
    "[2] Peter Harrington, 2012. Machine Learning in Action. Shelter Island, NY: Manning Publications Co."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
